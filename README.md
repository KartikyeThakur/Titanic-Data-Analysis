# Titanic-Data-Analysis

This project focuses on the Titanic dataset from Kaggle, performing thorough data cleaning and exploratory data analysis (EDA). The primary objectives are to handle missing values, correct inconsistencies, and format data appropriately to ensure it is suitable for analysis. In the EDA phase, the project investigates and visualizes data to uncover underlying patterns, relationships, and trends. By examining variables such as passenger demographics (age, gender, class), ticket fare, and embarkation point, the project aims to identify factors that significantly influenced survival outcomes. Through EDA, we can generate insights into how different features impacted the likelihood of survival, providing a comprehensive understanding of the dataset and guiding future predictive modeling efforts

## Acknowledgements
We would like to acknowledge the following resources and libraries that made this project possible:
- **pandas**: For data manipulation and analysis.
- **pandas_profiling**: For generating comprehensive EDA reports.
- **Kaggle**: For providing the Titanic dataset, which is available [here](https://www.kaggle.com/c/titanic/data).

## API Reference
This project does not utilize any external APIs. All data manipulation and analysis are performed using `pandas` and `pandas_profiling`.

## Color Reference
The visualizations and reports generated by `pandas_profiling` use the default color schemes provided by the library, ensuring a professional and consistent appearance throughout the analysis.

## Deployment
To deploy the Titanic Data Analysis project, follow these steps:

1. **Environment Setup**:
   - Ensure you have Python installed on your system. You can download it from [python.org](https://www.python.org/).
   - Install the necessary libraries using `pip`. The primary libraries required are `pandas` and `pandas_profiling`.

2. **Install Dependencies**:
    ```bash
    pip install pandas pandas-profiling
    ```

3. **Run the Script**:
   - Execute the main script `main.py` to perform the data analysis and generate the reports. The script will produce an HTML file with the profile report and a text file summarizing the missing values.
    ```bash
    python main.py
    ```

## Environment Variables
This project does not require any specific environment variables. All configurations, such as file paths and report titles, are handled within the script itself.

## Features
Key features of the Titanic Data Analysis project include:

1. **Data Loading**:
   - The script includes a function (`load_data`) that loads the Titanic dataset from a CSV file into a pandas DataFrame. This function simplifies the process of reading and managing data.

2. **Data Cleaning**:
   - A function (`clean_data`) identifies and handles missing values within the dataset. Missing values are replaced with a placeholder ("Not Available"), ensuring that the dataset is ready for analysis without any null values.

3. **Data Display**:
   - The script contains a function (`display_dataframe`) that displays the first few rows of both the original and cleaned datasets. This provides a quick overview of the data and helps verify that it has been loaded and cleaned correctly.

4. **Profile Report**:
   - Using `pandas_profiling`, the script generates an exploratory data analysis report. This report provides comprehensive insights into the dataset, including data distributions, correlations, and potential issues. The report is saved as an HTML file for easy viewing.

5. **Missing Values Summary**:
   - A summary of missing values in each column is saved to a text file. This summary helps in understanding the extent of missing data and can be useful for further data cleaning or imputation processes.

## Optimizations
Several potential optimizations and improvements can be made to enhance the project:

1. **Advanced Data Cleaning**:
   - Implement more sophisticated techniques for handling missing values, such as using statistical methods or machine learning models for imputation.
   - Address other data quality issues like duplicates, outliers, and inconsistent data types.

2. **Interactive Visualization**:
   - Integrate interactive plots and dashboards using libraries such as `Plotly`, `Dash`, or `Bokeh`. This would allow users to interact with the data visualizations and gain deeper insights.

3. **Parameterization**:
   - Modify the script to accept dynamic input for file paths, report titles, and other parameters. This would make the script more flexible and user-friendly, allowing it to be easily adapted for different datasets.

## Run Locally
To run the Titanic Data Analysis project on your local machine, follow these steps:

1. **Clone the Repository**:
    ```bash
    git clone https://github.com/your-repo/titanic-data-analysis.git
    ```

2. **Navigate to the Project Directory**:
    ```bash
    cd titanic-data-analysis
    ```

3. **Install Dependencies**:
    ```bash
    pip install pandas pandas-profiling
    ```

4. **Run the Main Script**:
    ```bash
    python main.py
    ```

   - This will execute the analysis workflow, generating the profile report (`titanic.html`) and the missing values summary (`missing_values_summary.txt`).

## Running Tests
Although this project does not currently include a dedicated testing suite, you can implement and run the following types of tests to ensure functionality:

1. **Unit Tests**:
   - Write unit tests for individual functions such as `load_data`, `clean_data`, and `display_dataframe`. These tests should verify that each function performs as expected.

2. **Integration Tests**:
   - Test the entire workflow from loading the dataset to generating the profile report and saving the summary of missing values. Ensure that all components work together seamlessly.

3. **Run Tests**:
   - If you use a testing framework like `pytest`, you can run your tests with the following command:
    ```bash
    pytest
    ```

   - This will execute all tests in the project and report any issues.

---

## Screenshots of the final output 
![Screenshot (70)](https://github.com/KartikyeThakur/Titanic-Data-Analysis/assets/172358250/3bf11b6d-ee33-4e51-8690-6e05cc2d4706)
![Screenshot (71)](https://github.com/KartikyeThakur/Titanic-Data-Analysis/assets/172358250/da66616c-053d-480c-9c90-656c412eaca8)
![Screenshot (72)](https://github.com/KartikyeThakur/Titanic-Data-Analysis/assets/172358250/ae4fd6e6-f13b-4cce-b1e1-1e1704d0e53a)
![Screenshot (73)](https://github.com/KartikyeThakur/Titanic-Data-Analysis/assets/172358250/cbf829b6-5ef6-49fc-9467-2f9c30c45f26)
![Screenshot (74)](https://github.com/KartikyeThakur/Titanic-Data-Analysis/assets/172358250/d7bb1aed-bc63-45b9-acf6-05bc7f370260)
![Screenshot (75)](https://github.com/KartikyeThakur/Titanic-Data-Analysis/assets/172358250/57895a73-0f1d-4d4e-86fa-0334a3719dab)
![Screenshot (76)](https://github.com/KartikyeThakur/Titanic-Data-Analysis/assets/172358250/37bd693e-bfec-48e6-8af0-1fef549fa991)
![Screenshot (77)](https://github.com/KartikyeThakur/Titanic-Data-Analysis/assets/172358250/910064cc-b3c1-4430-b951-4f56b5963ee9)
![Screenshot (78)](https://github.com/KartikyeThakur/Titanic-Data-Analysis/assets/172358250/22555eb8-d3a3-4526-b493-8739440bc25b)


